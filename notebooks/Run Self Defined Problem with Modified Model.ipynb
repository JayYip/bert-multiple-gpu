{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model Architecture\n",
    "\n",
    "By default, the model only contains BERT model and a dense layer for each problem. If you want to add things between BERT and dense layers, you can modify hidden method of BertMultiTask class. Here's an example of adding a cudnn GRU on top of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from bert_multitask_learning import (get_or_make_label_encoder, FullTokenizer, \n",
    "                                     create_single_problem_generator, train_bert_multitask, \n",
    "                                     eval_bert_multitask, DynamicBatchSizeParams, TRAIN, EVAL, PREDICT, BertMultiTask)\n",
    "import pickle\n",
    "import types\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new problem\n",
    "new_problem_type = {'imdb_cls': 'cls'}\n",
    "\n",
    "def imdb_cls(params, mode):\n",
    "    tokenizer = FullTokenizer(vocab_file=params.vocab_file)\n",
    "    \n",
    "    # get data\n",
    "    (train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "    label_encoder = get_or_make_label_encoder(params, 'imdb_cls', mode, train_labels+test_labels)\n",
    "    word_to_id = keras.datasets.imdb.get_word_index()\n",
    "    index_from=3\n",
    "    word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
    "    word_to_id[\"<PAD>\"] = 0\n",
    "    word_to_id[\"<START>\"] = 1\n",
    "    word_to_id[\"<UNK>\"] = 2\n",
    "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "    train_data = [[id_to_word[i] for i in sentence] for sentence in train_data]\n",
    "    test_data = [[id_to_word[i] for i in sentence] for sentence in test_data]\n",
    "    \n",
    "    if mode == TRAIN:\n",
    "        input_list = train_data\n",
    "        target_list = train_labels\n",
    "    else:\n",
    "        input_list = test_data\n",
    "        target_list = test_labels\n",
    "    \n",
    "    if mode == PREDICT:\n",
    "        return input_list, target_list, label_encoder\n",
    "        \n",
    "    return create_single_problem_generator('imdb_cls', input_list, target_list, label_encoder, params, tokenizer, mode)\n",
    "\n",
    "new_problem_process_fn_dict = {'imdb_cls': imdb_cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create params and model\n",
    "params = DynamicBatchSizeParams()\n",
    "params.init_checkpoint = 'models/cased_L-12_H-768_A-12'\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "model = BertMultiTask(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cudnngru_hidden(self, features, hidden_feature, mode):\n",
    "    # with shape (batch_size, seq_len, hidden_size)\n",
    "    seq_hidden_feature = hidden_feature['seq']\n",
    "    \n",
    "    cudnn_gru_layer = tf.keras.layers.CuDNNGRU(\n",
    "            units=self.params.bert_config.hidden_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "    )\n",
    "    gru_logit = cudnn_gru_layer(seq_hidden_feature)\n",
    "    \n",
    "    return_features = {}\n",
    "    return_hidden_feature = {}\n",
    "    \n",
    "    for problem_dict in self.params.run_problem_list:\n",
    "        for problem in problem_dict:\n",
    "            # for slightly faster training\n",
    "            return_features[problem], return_hidden_feature[problem] = self.get_features_for_problem(\n",
    "                    features, hidden_feature, problem, mode)\n",
    "    return return_features, return_hidden_feature\n",
    "\n",
    "model.hidden = types.MethodType(cudnngru_hidden, model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "train_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
    "                     num_epochs=10, params=params, \n",
    "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict, \n",
    "                     model=model, model_dir='models/ibdm_gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "print(eval_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
    "                     params=params, eval_scheme='acc',\n",
    "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict,\n",
    "                     model_dir='models/idbm_gru', model = model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
